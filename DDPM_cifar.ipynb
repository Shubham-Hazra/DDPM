{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import logging\n",
    "import numpy as np\n",
    "import torchvision \n",
    "import random,math\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from operator import attrgetter\n",
    "from functools import partial\n",
    "from torch.utils.data import DataLoader,default_collate,Dataset\n",
    "from copy import copy\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from collections.abc import Mapping\n",
    "from diffusers import UNet2DModel\n",
    "from tqdm import tqdm\n",
    "from diffusers import DDIMScheduler, DDPMScheduler\n",
    "from torch.optim.lr_scheduler import ExponentialLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the device\n",
    "def_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "# Function to send data to device\n",
    "def to_device(x, device=def_device):\n",
    "    if isinstance(x, torch.Tensor): return x.to(device)\n",
    "    if isinstance(x, Mapping): return {k:v.to(device) for k,v in x.items()}\n",
    "    return type(x)(to_device(o, device) for o in x)\n",
    "\n",
    "def to_cpu(x):\n",
    "    if isinstance(x, Mapping): return {k:to_cpu(v) for k,v in x.items()}\n",
    "    if isinstance(x, list): return [to_cpu(o) for o in x]\n",
    "    if isinstance(x, tuple): return tuple(to_cpu(list(x)))\n",
    "    res = x.detach().cpu()\n",
    "    return res.float() if res.dtype==torch.float16 else res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "# Remember to pad the images with 2 pixels on each side i.e. to make the image size 32x32\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "transforms = T.Compose([T.ToTensor(),lambda x: x - 0.5])\n",
    "\n",
    "train_ds = torchvision.datasets.CIFAR10(root = './data/train',train = True,download = True,transform = transforms)\n",
    "valid_ds = torchvision.datasets.CIFAR10(root = './data/valid',train = False,download = True,transform = transforms)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dl))\n",
    "x,y = batch\n",
    "print(x.shape,y.shape)\n",
    "\n",
    "images = x[:16] + 0.5\n",
    "grid_img = torchvision.utils.make_grid(images, nrow=4)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(np.transpose(grid_img, (1,2,0)))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPM_model(nn.Module):\n",
    "    def __init__(self, model,beta_min = 0.0001, beta_max = 0.02, n_steps = 1000):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.beta_min = beta_min\n",
    "        self.beta_max = beta_max\n",
    "        self.n_steps = n_steps\n",
    "        self.beta = torch.linspace(beta_min, beta_max, self.n_steps)\n",
    "        self.alpha = 1. - self.beta\n",
    "        self.alphabar = self.alpha.cumprod(dim=0)\n",
    "        self.sigma = self.beta.sqrt()\n",
    "\n",
    "    def add_noise(self, x_0):\n",
    "        device = x_0.device\n",
    "        n = len(x_0)\n",
    "        timesteps = torch.randint(0, self.n_steps, (n,), device=device)\n",
    "        alphabar_t = self.alphabar[timesteps].reshape(-1, 1, 1, 1).to(device)\n",
    "        noise = torch.randn_like(x_0, device=device)\n",
    "        x_t = x_0 * alphabar_t.sqrt() + (1. - alphabar_t).sqrt()* noise\n",
    "        return (x_t, timesteps.to(device)), noise\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model.forward(*x).sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(sched,model,sz = (16,3,32,32)):\n",
    "    preds = []\n",
    "    x_t = torch.randn(sz).cuda()\n",
    "    for t in tqdm(sched.timesteps,total=len(sched.timesteps)):\n",
    "        with torch.no_grad(): noise = model((x_t, t))\n",
    "        x_t = sched.step(noise, t, x_t).prev_sample\n",
    "        preds.append(x_t.float().cpu())\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model = UNet2DModel(in_channels=3, out_channels=3)\n",
    "model = DDPM_model(unet_model, n_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "lr = 0.005\n",
    "opt = optim.Adam(model.parameters(), lr=lr,eps = 1e-5)\n",
    "sched = optim.lr_scheduler.OneCycleLR(opt, lr, epochs=epochs, steps_per_epoch=len(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model,train_dl,valid_dl,loss_func,epochs,opt,sched):\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    for epoch in range(epochs):\n",
    "        losses = []\n",
    "        model.train()\n",
    "        for xb,_ in train_dl:\n",
    "            opt.zero_grad()\n",
    "            xb = to_device(xb)\n",
    "            (x_t, timesteps), noise = model.add_noise(xb)\n",
    "            (x_t, timesteps) = to_device((x_t, timesteps))\n",
    "            noise = to_device(noise)\n",
    "            with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "                preds = model((x_t, timesteps))\n",
    "                loss = loss_func(preds, noise)\n",
    "            losses.append(loss.item() * len(xb))\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(opt)\n",
    "            sched.step()\n",
    "            scaler.update()\n",
    "        print(f\"Epoch {epoch} train loss: {sum(losses)/len(train_ds)}\")\n",
    "        model.eval()\n",
    "        losses = []\n",
    "        with torch.no_grad():\n",
    "            for xb,_ in valid_dl:\n",
    "                xb = to_device(xb)\n",
    "                (x_t, timesteps), noise = model.add_noise(xb)\n",
    "                (x_t, timesteps) = to_device((x_t, timesteps))\n",
    "                noise = to_device(noise)\n",
    "                preds = model((x_t, timesteps))\n",
    "                loss = loss_func(preds, noise)\n",
    "                losses.append(loss.item() * len(xb))\n",
    "        print(f\"Epoch {epoch} valid loss: {sum(losses)/len(valid_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit(model.to(def_device),train_dl,valid_dl,F.mse_loss,epochs,opt,sched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model.cpu()\n",
    "# torch.save(model.state_dict(),\"DDPM_cifar_state_dict.pth\")\n",
    "# torch.save(model.model.state_dict(),\"DDPM_cifar_unet_state_dict.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DDPM_model(unet_model,1000)\n",
    "model.load_state_dict(torch.load(\"DDPM_cifar_state_dict.pth\",map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dl))\n",
    "x_0 = batch[0][:16]+ 0.5\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.suptitle('Original images')\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    plt.imshow(x_0[i].permute(1,2,0))\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "sched = DDIMScheduler()\n",
    "sched.set_timesteps(100)\n",
    "preds = sample(sched,model)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.suptitle(\"Generated images using skip sampling method 1\")\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    plt.imshow(preds[-1][i].permute(1,2,0) + 0.5)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def get_images(preds, num , l):\n",
    "    return [preds[i][num].permute(1,2,0) + 0.5 for i in l]\n",
    "\n",
    "def animate(images):\n",
    "    fig = plt.figure(figsize=(3, 3))\n",
    "    plt.axis(\"off\")\n",
    "    ims = [[plt.imshow(image, animated=True, cmap='gray')] for i, image in enumerate(images)]\n",
    "    plt.close()\n",
    "    ani = animation.ArtistAnimation(fig, ims, interval=50, blit=True, repeat_delay=3000)\n",
    "    display(HTML(ani.to_html5_video()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = random.randint(0, preds[0].shape[0]-1)\n",
    "l = list(range(0,len(preds)))\n",
    "images = get_images(preds,num,l)\n",
    "animate(images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
